{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a17d7c5",
   "metadata": {},
   "source": [
    "# Chorale Rearranger\n",
    "\n",
    "For this assignment, we were interested in the direct difference between the information offered by midi and audio data, namely the information about a piece of music that you can access from a Midi data that is difficult or impossible to obtain directly from an audio file. So we decided upon a project that would use this information (namely the note start and end times and the pitch values), to edit an audio file.\n",
    "\n",
    "We decided to look at the chorale, a form of music with clearly defined melodic lines, yet where all of the lines blend together so that it becomes difficult to ascertain the lines individually. However, in a midi data, we would have all of this information from each line directly. So we decided to try and segment, alter, and rearrange the individual lines of a recording of the third movement of Bach's MatthÃ¤uspassion (https://www.youtube.com/watch?v=1AnEuCJpcCE) based upon a midi file that we found (https://www.cpdl.org/wiki/images/2/26/Ws-bwv-mp03.mid).\n",
    "\n",
    "We would slice into the audio based upon the start and end times of the midi notes, and then apply a tight band pass filter around the pitch of each midi note. However, we would repeat this process four times, in each one altering the the centre frequency of the band pass. For the first version, we would centre on the the pitch of the corresponding voice, however, with each repition we would shift the voice from which we were taking the pitch values by one, so for example, the Soprano would firstly have the soprano pitches, then the alto pitches, then the tenor and the bass.\n",
    "\n",
    "The idea is that the melodic lines would remain relatively similar, however the harmonies would shift around, between consonance and dissonance.\n",
    "\n",
    "We would then generate a verison of the resulting piece with sine waves, that we would underlay the recording with, so as to keep the tonality pretty clear.\n",
    "\n",
    "After that we would send the piece to Pure Data for some processing, and then return to Python to apply some reverb via convolution.\n",
    "\n",
    "The result is an ambient version of the chorale repeated four times, with some clear melodic lines, but with a textural background that shifts between consonance and disonance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ca6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import math\n",
    "import scipy\n",
    "import time\n",
    "from scipy import signal\n",
    "import pretty_midi as pm\n",
    "import soundfile as sf\n",
    "import os\n",
    "from matplotlib import colors\n",
    "from scipy import interpolate\n",
    "\n",
    "sr = 48000\n",
    "\n",
    "# function to convert freq in Hz to normalised frequency\n",
    "    \n",
    "def freqNormaliser(f):\n",
    "    om = f/(sr*0.5)\n",
    "    return om\n",
    "\n",
    "# function to convert from Hz to Midi note value\n",
    "\n",
    "def herztoMidi(f):\n",
    "    midi =  np.around((12*np.log2(f/440) + 69), 0)\n",
    "    return midi\n",
    "\n",
    "# function to find the index of the nearst value in an array to an input\n",
    "# (from https://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array)\n",
    "\n",
    "def nearestValue(array, value):\n",
    "    dif = (abs(array-value)).argmin()\n",
    "    return dif\n",
    "\n",
    "# function to create a window with a sigmoid ramp shape\n",
    "# this is used when appending audio with overlap\n",
    "# chosen over hann, as hann still led to excesive amplitude modulations\n",
    "# takes two arguments (N = size of window, c = length of ramp)\n",
    "\n",
    "# got sigmoid code courtesy of Joachim:\n",
    "        #z = np.exp(-i)\n",
    "        #sig = 1 / (1 + z)\n",
    "\n",
    "\n",
    "def sigmoidWindow(N,c):\n",
    "    \n",
    "    rampup = np.zeros(1)\n",
    "    \n",
    "    for i in range(c):\n",
    "\n",
    "        z = np.exp(-i)\n",
    "        sig = 1 / (1 + z)\n",
    "        rampup = np.append(rampup, sig)\n",
    "    \n",
    "    window = np.concatenate((rampup, np.ones((N-2*c)-2), np.flip(rampup)))\n",
    "        \n",
    "\n",
    "    return window\n",
    "\n",
    "# sine wave generation function\n",
    "\n",
    "def sin_generator(amp, f, t):\n",
    "    return amp*np.sin(2*np.pi*f*t)\n",
    "\n",
    "# tight band pass filter around a frequency\n",
    "# used to cut into the audio for each segment\n",
    "# q arguement can be used to set pass band size\n",
    "# IIR filter chosen as runs faster than FIR and will be run a lot!\n",
    "\n",
    "def voiceFilter(f, song, q):\n",
    "    \n",
    "    Nband, Wband = signal.buttord([freqNormaliser(f-1), freqNormaliser(f+1)], [freqNormaliser(f-q), freqNormaliser(f+q)], 1, 10)\n",
    "    bband, aband = signal.butter(N = Nband, Wn = Wband, btype='bandpass')\n",
    "    filteredaudio = signal.lfilter(bband, aband, song)\n",
    "    \n",
    "    return filteredaudio\n",
    "\n",
    "# a really bad saturator\n",
    "\n",
    "def badSaturator(sigin, level):\n",
    "    \n",
    "    for i in range(sigin.size):\n",
    "        \n",
    "        if sigin[i] >= level:\n",
    "            sigin[i] = level\n",
    "\n",
    "        if sigin[i] <= -level:\n",
    "            sigin[i] = -level\n",
    "            \n",
    "    return sigin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f32f3",
   "metadata": {},
   "source": [
    "# Step 1 : Import the Audio File\n",
    "\n",
    "The audio data used is taken from this YouTube video:\n",
    "\n",
    "https://www.youtube.com/watch?v=1AnEuCJpcCE\n",
    "\n",
    "We were not so concerned that we would have a low bandwidth audio file, no information above 10 kHz, as this area would be filted out anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22471f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = './Files/bach.wav'\n",
    "\n",
    "bach, sr = librosa.load(data, sr = sr)\n",
    "\n",
    "ipd.Audio(bach, rate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4da3a8",
   "metadata": {},
   "source": [
    "# Step 2: Import the Midi Data\n",
    "\n",
    "The Midi data used is taken from:\n",
    "\n",
    "https://www.cpdl.org/wiki/index.php/Matth%C3%A4uspassion,_BWV_244_(Johann_Sebastian_Bach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61229fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_bach = pm.PrettyMIDI('./Files/bach.mid')\n",
    "\n",
    "midilisten = midi_bach.synthesize(fs=sr)\n",
    "\n",
    "ipd.Audio(midilisten, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bce6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set each of the voices to be in their own variable, so that we can access data such as the starts,\n",
    "# ends, and pitches of each note.\n",
    "\n",
    "soprano = midi_bach.instruments[0]\n",
    "alto = midi_bach.instruments[1]\n",
    "tenor = midi_bach.instruments[2]\n",
    "bass = midi_bach.instruments[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436f520",
   "metadata": {},
   "source": [
    "The biggest problem with our plan for slicing out of the audio between the starts and the ends of the Midi notes, is that currently the audio and the Midi do not allign in timing, tempo, or pitch. The first few steps to take are to reallign the audio and Midi so that they are a lot closer together. Even though it is next to impossible to get them to allign perfectly in the time given for the assignment, we were able to reach a pretty good standard that is enough for the purposes here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd11277",
   "metadata": {},
   "source": [
    "# Step 3: Reconcile the Pitches of the Audio and the Midi Data\n",
    "\n",
    "Through listening back to the audio and the synthesized Midi, it is possible to hear that both are in different keys. This need to be reconciled. It's possible to do this by ear in a DAW, but we aimed to do this within Python. We based this reconciliation upon the fact that the objective key of each doesn't matter, but rather the difference in pitch between them. So if we can make the pitches of the first chord in the Midi match the pitches in the first chord of the audio, we can then just subtract that value from all of the Midi note pitches.\n",
    "\n",
    "The method that we are going to use is the following:\n",
    "\n",
    "1. Find the rough duration of the first chord from the Midi data.\n",
    "2. Cut this out of the audio.\n",
    "3. Perform a Periodogram on the audio, and find the peak in this spectrum. This peak will most likely correspond to the fundamental of one of the voices.\n",
    "4. Compare this to the 4 Midi pitch values of each voice in the first chord and find the nearest one.\n",
    "5. Subtract these two values to find the difference.\n",
    "6. Subtract this difference from all Midi pitches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db18e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists to dump all of the starting and ending values of the first notes into\n",
    "\n",
    "firstchordstartlist = []\n",
    "firstchordendlist = []\n",
    "\n",
    "# find start and end of first notes of soprano and dump into lists\n",
    "\n",
    "firstchordstartsop = soprano.notes[0].start*sr\n",
    "firstchordstartlist.append(firstchordstartsop)\n",
    "firstchordendsop = soprano.notes[0].end*sr\n",
    "firstchordendlist.append(firstchordendsop)\n",
    "\n",
    "# find start and end of first notes of alto and dump into lists\n",
    "\n",
    "firstchordstartalt = alto.notes[0].start*sr\n",
    "firstchordstartlist.append(firstchordstartalt)\n",
    "firstchordendalt = alto.notes[0].end*sr\n",
    "firstchordendlist.append(firstchordendalt)\n",
    "\n",
    "# find start and end of first notes of tenor and dump into lists\n",
    "\n",
    "firstchordstartten = tenor.notes[0].start*sr\n",
    "firstchordstartlist.append(firstchordstartten)\n",
    "firstchordendten = tenor.notes[0].end*sr\n",
    "firstchordendlist.append(firstchordendten)\n",
    "\n",
    "# find start and end of first notes of bass and dump into lists\n",
    "\n",
    "firstchordstartbas = bass.notes[0].start*sr\n",
    "firstchordstartlist.append(firstchordstartbas)\n",
    "firstchordendbas = bass.notes[0].end*sr\n",
    "firstchordendlist.append(firstchordendbas)\n",
    "\n",
    "# Now get the min from the start and max from the end to ensure we get the entirety of the first chord\n",
    "\n",
    "firstchordstart = min(firstchordstartlist)\n",
    "firstchordend = min(firstchordendlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get spectrum of audio on the first chord\n",
    "\n",
    "freq, psd = signal.periodogram(bach[int(firstchordstart):int(firstchordend)], sr)\n",
    "\n",
    "# find the peak in this spectrum\n",
    "\n",
    "peak = librosa.util.peak_pick(psd, 0, psd.size, 0, psd.size, psd.max()*0.99, 10)\n",
    "\n",
    "# convert this to a Midi value\n",
    "\n",
    "audiofundamental = herztoMidi(freq[peak])\n",
    "\n",
    "# Plot the Periodogran with the peak identified\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.title('Periodogram of First Chord in Audio')\n",
    "plt.xlabel('Frequency (Hz (Logarithmic))')\n",
    "plt.ylabel('Spectral Density (V*rms^2/Hz)')\n",
    "plt.vlines(peak, 0, psd[peak]+(psd[peak]*0.1), linestyles =\"dashed\", colors =\"r\")\n",
    "plt.semilogx(freq, psd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the Midi pitches of the first notes\n",
    "\n",
    "firstnotepitches = [soprano.notes[0].pitch, alto.notes[0].pitch, tenor.notes[0].pitch, bass.notes[0].pitch]\n",
    "\n",
    "# find the nearest value in the list to the peak in the audio\n",
    "\n",
    "midifundamental = firstnotepitches[nearestValue(firstnotepitches, audiofundamental)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76620556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the difference between them from all of the Midi pitch values\n",
    "\n",
    "for i, j, k, l in zip(soprano.notes, alto.notes, tenor.notes, bass.notes):\n",
    "    i.pitch = i.pitch + (audiofundamental - midifundamental)\n",
    "    j.pitch = j.pitch + (audiofundamental - midifundamental)\n",
    "    k.pitch = k.pitch + (audiofundamental - midifundamental)\n",
    "    l.pitch = l.pitch + (audiofundamental - midifundamental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b32f36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now to listen to the synthesis of the new altered Midi data\n",
    "\n",
    "synth = midi_bach.synthesize(fs = sr)\n",
    "ipd.Audio(synth, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c992814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can play it back on top of the audio to compare\n",
    "\n",
    "kombi = bach + synth[:bach.size]\n",
    "\n",
    "ipd.Audio(kombi, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372e1c8",
   "metadata": {},
   "source": [
    "We can hear that these are now both in the same key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bca011",
   "metadata": {},
   "source": [
    "# Step 4: Remove Silence at Start\n",
    "\n",
    "We can hear that there is also a few seconds of silence at the start of both the Midi and the audio data. We can remove this so that the music begins immediately on playback. We will first remove this from the audio and then from the Midi.\n",
    "\n",
    "For the audio we will find the envelope and then the peaks of the envelope. We will then scale these to the values of the audio data. As there will be a peak at the start of the audio data, we will then remove the section between the first and second peak, resulting in the audio starting with the first note.\n",
    "\n",
    "For the Midi, we will just minus the start time of the first note from all start and end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c144a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an onset envelop of the audio data\n",
    "\n",
    "onset_env = librosa.onset.onset_strength(y=bach, sr=sr,hop_length=512,aggregate=np.median)\n",
    "onset_env = onset_env/onset_env.max()\n",
    "\n",
    "# find some peaks (doesn't really matter how many and where, as long as the peak at the start of the audio\n",
    "# data and the peak at the start of the music are roughly caught)\n",
    "\n",
    "peaks = librosa.util.peak_pick(onset_env[0:onset_env.size], 100, 100, 100, 100, 0.25, 100)\n",
    "\n",
    "# plot the onset envelope with the peaks\n",
    "\n",
    "plt.figure(figsize = (14, 3))\n",
    "plt.title('Onset Envelope of the Audio Data')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Normalised Strength')\n",
    "plt.vlines(peaks, 0, onset_env.max()+onset_env.max()*0.1, linestyles =\"dashed\", colors =\"r\")\n",
    "\n",
    "plt.plot(onset_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15664e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale peaks to the values in the audio data\n",
    "\n",
    "peakscaler = int(bach.size/onset_env.size)\n",
    "peaksscaled = peaks*peakscaler\n",
    "times = (np.arange(0, bach.size, 1)/sr)\n",
    "\n",
    "# plot the peaks on the waveform\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Peaks on Waveform')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.vlines(peaksscaled/sr, -0.5, 0.5, linestyles =\"dashed\", colors =\"r\")\n",
    "plt.plot(times, bach)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eea41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the section between the first and second peak from the audio data and listen\n",
    "\n",
    "bach = bach[peaksscaled[1]:bach.size+1]\n",
    "\n",
    "ipd.Audio(bach, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the silence at the beginning of the Midi data\n",
    "\n",
    "# create a list of all start times of each voice\n",
    "\n",
    "midistartslist = [soprano.notes[0].start, alto.notes[0].start, tenor.notes[0].start, bass.notes[0].start]\n",
    "\n",
    "# find the start of the very first note\n",
    "\n",
    "chopchop = min(midistartslist)\n",
    "\n",
    "# remove from all start and end times\n",
    "\n",
    "for i, j, k, l in zip(soprano.notes, alto.notes, tenor.notes, bass.notes):\n",
    "    i.start = i.start - chopchop\n",
    "    i.end = i.end - chopchop\n",
    "    j.start = j.start - chopchop\n",
    "    j.end = j.end - chopchop\n",
    "    k.start = k.start - chopchop\n",
    "    k.end = k.end - chopchop\n",
    "    l.start = l.start - chopchop\n",
    "    l.end = l.end - chopchop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb86290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize and listen back\n",
    "\n",
    "synth = midi_bach.synthesize(fs = sr)\n",
    "\n",
    "ipd.Audio(synth, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5e609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can combine the audio and the synthesized Midi again and listen back\n",
    "\n",
    "kombi = bach + synth[:bach.size]\n",
    "\n",
    "ipd.Audio(kombi, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88226fc7",
   "metadata": {},
   "source": [
    "What's noticable here is that the audio and the midi are performed at different tempi. Reconciling this difference is what we will do next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c1918",
   "metadata": {},
   "source": [
    "# Step 5: Reconcile Tempi\n",
    "\n",
    "Similar to the pitch, the objective tempi of the audio and the Midi are not so important here, but rather the difference between them. Both librosa and pretty Midi have a tempo estimation function which will form the basis of the reconciliation. We will find the difference between these estimations and then scale the durations of the Midi notes accordingly (this is a very rough method and doesn't offer perfect results, but it works well enough for the purposes here).\n",
    "\n",
    "The method we will use is as follows:\n",
    "\n",
    "1. Estimate the tempi of audio and Midi\n",
    "2. Find a scale factor by dividing the Midi tempo by the audio tempo.\n",
    "3. Find the durations of the Midi notes by subtracting end time from start time for each note and put them into an array.\n",
    "4. Multiply the array by the scale factor to get the new durations of each note.\n",
    "5. Go through the Midi data and change the start values and end values based upon these durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate tempi of audio/midi\n",
    "\n",
    "bachtempo = float(librosa.beat.tempo(bach, sr=sr))\n",
    "\n",
    "miditempo = midi_bach.estimate_tempo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bb876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find scale factor\n",
    "\n",
    "scalefactor = miditempo/bachtempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92558bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get note durations\n",
    "\n",
    "sopranodurations = np.empty(0)\n",
    "altodurations = np.empty(0)\n",
    "tenordurations = np.empty(0)\n",
    "bassdurations = np.empty(0)\n",
    "\n",
    "#can't zip here cause different lengths\n",
    "\n",
    "for i in soprano.notes:\n",
    "    sopranodurations = np.append(sopranodurations, (i.end - i.start))\n",
    "    \n",
    "for i in alto.notes:\n",
    "    altodurations = np.append(altodurations, (i.end - i.start))\n",
    "    \n",
    "for i in tenor.notes:\n",
    "    tenordurations = np.append(tenordurations, (i.end - i.start))\n",
    "    \n",
    "for i in bass.notes:\n",
    "    bassdurations = np.append(bassdurations, (i.end - i.start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba78859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiply  note durations by scale factor\n",
    "\n",
    "sopranodurations = sopranodurations*scalefactor\n",
    "altodurations = altodurations*scalefactor\n",
    "tenordurations = tenordurations*scalefactor\n",
    "bassdurations = bassdurations*scalefactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c571bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out start values\n",
    "\n",
    "sopranostarts = np.empty(0)\n",
    "altostarts = np.empty(0)\n",
    "tenorstarts = np.empty(0)\n",
    "bassstarts = np.empty(0)\n",
    "\n",
    "for i in soprano.notes:\n",
    "    sopranostarts = np.append(sopranostarts, i.start)\n",
    "\n",
    "for i in alto.notes:\n",
    "    altostarts = np.append(altostarts, i.start)\n",
    "\n",
    "for i in tenor.notes:\n",
    "    tenorstarts = np.append(tenorstarts, i.start)\n",
    "\n",
    "for i in bass.notes:\n",
    "    bassstarts = np.append(bassstarts, i.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the note lengths to change tempo\n",
    "\n",
    "for i, j in enumerate(soprano.notes):\n",
    "    j.end = sopranostarts[i]+sopranodurations[i]\n",
    "    j.start = sopranostarts[i]\n",
    "    if i+1 == len(soprano.notes):\n",
    "        break\n",
    "    sopranostarts[i+1] = sopranostarts[i]+sopranodurations[i]\n",
    "\n",
    "for i, j in enumerate(alto.notes):\n",
    "    j.end = altostarts[i]+altodurations[i]\n",
    "    j.start = altostarts[i]\n",
    "    if i+1 == len(alto.notes):\n",
    "        break\n",
    "    altostarts[i+1] = altostarts[i]+altodurations[i]\n",
    "\n",
    "for i, j in enumerate(tenor.notes):\n",
    "    j.end = tenorstarts[i]+tenordurations[i]\n",
    "    j.start = tenorstarts[i]\n",
    "    if i+1 == len(tenor.notes):\n",
    "        break\n",
    "    tenorstarts[i+1] = tenorstarts[i]+tenordurations[i]\n",
    "\n",
    "for i, j in enumerate(bass.notes):\n",
    "    j.end = bassstarts[i]+bassdurations[i]\n",
    "    j.start = bassstarts[i]\n",
    "    if i+1 == len(bass.notes):\n",
    "        break\n",
    "    bassstarts[i+1] = bassstarts[i]+bassdurations[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can synthesize the Midi and listen back\n",
    "\n",
    "synth = midi_bach.synthesize(fs = sr)\n",
    "ipd.Audio(synth, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b211a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and listen to it compared to the audio\n",
    "\n",
    "bachkombi = bach + (np.append(synth, np.zeros(bach.size-synth.size)))\n",
    "\n",
    "ipd.Audio(bachkombi, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6d743",
   "metadata": {},
   "source": [
    "While this works to an extent, with the Midi and audio alligning perfectly in some sections, in others they are completely out of allignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37a5ac",
   "metadata": {},
   "source": [
    "# Step 6: Allign the Midi Notes with the Audio Notes\n",
    "\n",
    "There is one last method that we can attempt to put them into better allignment. From listening to the audio (and examining the waveform above), we can here that there are several expressive pauses between phrases. These are not found in the Midi data, with each phrase carrying directly on into the next. We will therefore attempt to line up the start of each phrase in the Midi with the start of each phrase in the audio, so that even if the drift out of allignment over the course of the phrase, at least they will come back into allignment at the start of the next.\n",
    "\n",
    "The method we will use is as follows:\n",
    "\n",
    "1. Find the start times of each of the phrases. This is done through the envelope and peak analysis method as used above, except here it keeps running through this analysis until only the start of each phrase is found. This is done by manually setting the number of phrases and running through until these values are found (ideally this would also be done automatically without having to manaully set it, but I don't have time or any ideas on how to do this at the moment).\n",
    "\n",
    "2. Scale the peak values from frames to seconds.\n",
    "\n",
    "3. Find the nearest note in each voice to the peak and calculate the difference between them.\n",
    "\n",
    "4. Add this difference onto the start and end values of each Midi note that comes after this peak.\n",
    "\n",
    "5. Repeat steps 3 and 4 for each peak value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aba507",
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofphrases = 4\n",
    "\n",
    "# find peaks in audio after silence to shift midi too\n",
    "\n",
    "onset_env = librosa.onset.onset_strength(y=bach, sr=sr,hop_length=512,aggregate=np.median)\n",
    "\n",
    "\n",
    "# pull in the amount of the envelope over which the peaks are searched for, while increasing the number of samples\n",
    "# over which the analysis is performed until only (numberofphrases)peaks remain.\n",
    "\n",
    "for i in range(onset_env.size):\n",
    "    peaks = librosa.util.peak_pick(onset_env[0+i:onset_env.size-i], i, 300, i, 300, 0.5, 800)\n",
    "    if peaks.size == numberofphrases:\n",
    "        break\n",
    "        \n",
    "# normalise the values in the onset envelope\n",
    "\n",
    "onset_env = onset_env/onset_env.max()\n",
    "\n",
    "# plot the peaks\n",
    "\n",
    "plt.figure(figsize = (14, 3))\n",
    "plt.title('Onset Envelope of the Audio Data')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Normalised Strength')\n",
    "plt.vlines(peaks, 0, onset_env.max()+onset_env.max()*0.1, linestyles =\"dashed\", colors =\"r\")\n",
    "plt.plot(onset_env)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will also plot theses on the waveform\n",
    "\n",
    "# scale peaks to the samples in the audio data\n",
    "\n",
    "peakscaler = int(bach.size/onset_env.size)\n",
    "peaksscaled = peaks*peakscaler\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Peaks on Waveform')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.vlines(peaksscaled/sr, -0.5, 0.5, linestyles =\"dashed\", colors =\"r\")\n",
    "plt.plot(np.arange(0, bach.size, 1)/sr, bach)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the times of the peaks from samples to seconds\n",
    "\n",
    "peaktimes = peaksscaled/sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in peaktimes:\n",
    "    \n",
    "    a = nearestValue(sopranostarts, i)\n",
    "    dif = i-sopranostarts[a]\n",
    "    \n",
    "    for j in range(a, len(sopranostarts)):\n",
    "        soprano.notes[j].start = soprano.notes[j].start+dif\n",
    "        soprano.notes[j].end = soprano.notes[j].end+dif\n",
    "        \n",
    "    a = nearestValue(altostarts, i)\n",
    "    dif = i-altostarts[a]\n",
    "    \n",
    "    for j in range(a, len(altostarts)):\n",
    "        alto.notes[j].start = alto.notes[j].start+dif\n",
    "        alto.notes[j].end = alto.notes[j].end+dif\n",
    "        \n",
    "    a = nearestValue(tenorstarts, i)\n",
    "    dif = i-tenorstarts[a]\n",
    "    \n",
    "    for j in range(a, len(tenorstarts)):\n",
    "        tenor.notes[j].start = tenor.notes[j].start+dif\n",
    "        tenor.notes[j].end = tenor.notes[j].end+dif\n",
    "        \n",
    "    a = nearestValue(bassstarts, i)\n",
    "    dif = i-bassstarts[a]\n",
    "    \n",
    "    for j in range(a, len(bassstarts)):\n",
    "        bass.notes[j].start = bass.notes[j].start+dif\n",
    "        bass.notes[j].end = bass.notes[j].end+dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc109b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can synthesize the Midi and listen back\n",
    "\n",
    "synth = midi_bach.synthesize(fs = sr)\n",
    "ipd.Audio(synth, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and listen to it compared to the audio\n",
    "\n",
    "bachkombi = bach + (np.append(synth, np.zeros(bach.size-synth.size)))\n",
    "\n",
    "ipd.Audio(bachkombi, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make the audio and the Midi the same length, we will extend the duration of the final notes so that they end\n",
    "# at the same point as the length of the audio data\n",
    "\n",
    "soprano.notes[len(soprano.notes)-1].end = bach.size/sr\n",
    "alto.notes[len(alto.notes)-1].end = bach.size/sr\n",
    "tenor.notes[len(tenor.notes)-1].end = bach.size/sr\n",
    "bass.notes[len(bass.notes)-1].end = bach.size/sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can synthesize the Midi and listen back\n",
    "\n",
    "synth = midi_bach.synthesize(fs = sr)\n",
    "ipd.Audio(synth, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ea82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and listen to it compared to the audio\n",
    "\n",
    "# pad the audio to make same length\n",
    "\n",
    "bach = np.concatenate((bach, np.zeros(synth.size-bach.size)))\n",
    "\n",
    "bachkombi = synth + bach\n",
    "\n",
    "ipd.Audio(bachkombi, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b3b35",
   "metadata": {},
   "source": [
    "The result is still not really extremely close to what we want, but due to issues of time we will have to move on from this.\n",
    "\n",
    "This has been an interesting experiment. The issue of reconciling the Midi with the audio was one that we did not really think about before starting, and has turned out to be a lot more difficult than expected. This could be something to work on for the improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c8316",
   "metadata": {},
   "source": [
    "# Step 7: Prepare the Data Needed for Rearranging Audio\n",
    "\n",
    "Before we start rearranging the audio, there are still a few arrays to be prepare of the information contained in the Midi notes. We are doing this so that it is easier to access this data directly instead of having to dive into the Midi note information.\n",
    "\n",
    "We will prepare arrays of start time, end times, durations, and pitches for each Note in each voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf53981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start values\n",
    "\n",
    "sopranostarts = np.empty(0)\n",
    "altostarts = np.empty(0)\n",
    "tenorstarts = np.empty(0)\n",
    "bassstarts = np.empty(0)\n",
    "\n",
    "for i in soprano.notes:\n",
    "    sopranostarts = np.append(sopranostarts, i.start)\n",
    "\n",
    "for i in alto.notes:\n",
    "    altostarts = np.append(altostarts, i.start)\n",
    "\n",
    "for i in tenor.notes:\n",
    "    tenorstarts = np.append(tenorstarts, i.start)\n",
    "\n",
    "for i in bass.notes:\n",
    "    bassstarts = np.append(bassstarts, i.start)\n",
    "    \n",
    "# end values\n",
    "\n",
    "sopranoends = np.empty(0)\n",
    "altoends = np.empty(0)\n",
    "tenorends = np.empty(0)\n",
    "bassends = np.empty(0)\n",
    "\n",
    "for i in soprano.notes:\n",
    "    sopranoends = np.append(sopranoends, i.end)\n",
    "\n",
    "for i in alto.notes:\n",
    "    altoends = np.append(altoends, i.end)\n",
    "\n",
    "for i in tenor.notes:\n",
    "    tenorends = np.append(tenorends, i.end)\n",
    "\n",
    "for i in bass.notes:\n",
    "    bassends = np.append(bassends, i.end)\n",
    "    \n",
    "# duration values\n",
    "\n",
    "sopranodurations = sopranoends - sopranostarts\n",
    "altodurations = altoends - altostarts\n",
    "tenordurations = tenorends - tenorstarts\n",
    "bassdurations = bassends - bassstarts\n",
    "\n",
    "# pitch values\n",
    "\n",
    "sopranopitches = np.empty(0)\n",
    "altopitches = np.empty(0)\n",
    "tenorpitches = np.empty(0)\n",
    "basspitches = np.empty(0)\n",
    "\n",
    "for i in soprano.notes:\n",
    "    sopranopitches = np.append(sopranopitches, (pm.note_number_to_hz(i.pitch)))\n",
    "\n",
    "for i in alto.notes:\n",
    "    altopitches = np.append(altopitches, (pm.note_number_to_hz(i.pitch)))\n",
    "\n",
    "for i in tenor.notes:\n",
    "    tenorpitches = np.append(tenorpitches, (pm.note_number_to_hz(i.pitch)))\n",
    "\n",
    "for i in bass.notes:\n",
    "    basspitches = np.append(basspitches, (pm.note_number_to_hz(i.pitch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1f4b8",
   "metadata": {},
   "source": [
    "# Step 8: Rearrange the Chorale\n",
    "\n",
    "Before we start rearring the chorale, there is a cell that creates several nests of the pitches array. Theses are to be accessed for the arrangement of each voice in the rearrangement cell. They run through pitches to be used by each voice in turn to create each variation. The pitches are offset by one voice in relation to the start and end times so that, for example, when arranging the soprano start and end times, the alto pitches are used.\n",
    "\n",
    "The reearranging of the chorale takes place in one large cell, that involves several steps.\n",
    "\n",
    "As an example we can look at what happens for the soprano start and end values.\n",
    "\n",
    "Firstly, there is an iteration of the whole code block between 0 and 3, corresponding to each of the pitches arrays in the nest. So the first time through, the soprano notes take the soprano pitches, the second time through the alto pitches, the third time through, the tenor pitches, and the fourth time through the bass pitches.\n",
    "\n",
    "Each time through, the audio is cut between the values of the start of the midi note and the end of the midi note. A very tight band pass filter is applied to this segment of the audio, focussing on the pitch of the midi note. With each variation, the pass band of this filter widens slightly. This is to allow more of the vocal to slowly come through. The audio is then passed through a saturator, and then a window function is applied. This segment is then concatenated onto the finished audio for this voice using an overlap technique.\n",
    "\n",
    "There is an issue that came up for which a solution had to be decided upon. As each of the voices has a different amount of notes, and the note values do not match directly to one another (and this is good, as otherwise each variation would be identical) a solution had to be found for which pitches to assign to which notes.\n",
    "\n",
    "The solution that was settled upon is that if there were more notes in the voice than in the voice from which the pitches were to be taken, the final pitch would just be repated for all of the excess notes. This is for a number of reasons: a) This would preserve the V-I resolution of the chorale, b) This would lead to greater variations over the course of the chorale while also preserving this resolution, c) This is the easiest to implement in code and time was short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the nests of pitches arrays to be used by each voice\n",
    "\n",
    "pitchesnest = [sopranopitches, altopitches, tenorpitches, basspitches]\n",
    "pitchesnest2 = [altopitches, tenorpitches, basspitches, sopranopitches]\n",
    "pitchesnest3 = [tenorpitches, basspitches, sopranopitches, altopitches]\n",
    "pitchesnest4 = [basspitches, sopranopitches, altopitches, tenorpitches]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd453b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the number of samples for the overlap between segments\n",
    "\n",
    "overlap = 64\n",
    "\n",
    "# declare the empty arrays onto which the segments will be concatenated\n",
    "   \n",
    "audio = np.empty(0)\n",
    "audio2 = np.empty(0)\n",
    "audio3 = np.empty(0)\n",
    "audio4 = np.empty(0)\n",
    "\n",
    "for z in range(0, 4):\n",
    "    \n",
    "    # declare the values used to ensure that the last pitch value is held for excess notes\n",
    "    \n",
    "    a = 1\n",
    "    b = 1\n",
    "    c = 1\n",
    "    d = 1\n",
    "    \n",
    "    # create the soprano line\n",
    "    \n",
    "    for i, j in enumerate(sopranostarts): \n",
    "        \n",
    "        # slice the audio between the start and end of each note\n",
    "    \n",
    "        bachcut = bach[int(j*sr):int(sopranoends[i]*sr)+1]\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest[z]):\n",
    "            i = i-a\n",
    "            a = a+1\n",
    "        else:\n",
    "            i = i\n",
    "            \n",
    "        # apply the filter. For each variation the pass band is widened.\n",
    "    \n",
    "        bachfilt = voiceFilter(pitchesnest[z][i], bachcut, z+20)\n",
    "    \n",
    "        # apply saturation\n",
    "        \n",
    "        bachfilt = bachfilt + badSaturator(bachfilt, bachfilt.max()/6)\n",
    "        \n",
    "        # apply sigmoid window\n",
    "        \n",
    "        bachfilt = bachfilt*sigmoidWindow(bachfilt.size, int((overlap)*(i+1)))\n",
    "        \n",
    "        # append the segment using overlap. The first segment has nothing to be overlapped with, hence the\n",
    "        # if statement\n",
    "        \n",
    "        if z == 0 and i == 0:\n",
    "    \n",
    "            audio = np.concatenate((audio, bachfilt))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            audio = np.concatenate((audio, np.zeros(bachfilt.size-overlap*(i+1))))\n",
    "            audio = np.concatenate((audio[:(audio.size-bachfilt.size)], (audio[(audio.size-bachfilt.size)] + bachfilt)))\n",
    "    \n",
    "    # create the alto line\n",
    "    \n",
    "    for i, j in enumerate(altostarts):\n",
    "        \n",
    "        # slice the audio between the start and end of each note\n",
    "    \n",
    "        bachcut = bach[int(j*sr):int(altoends[i]*sr)+1]\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest2[z]):\n",
    "            i = i-b\n",
    "            b = b+1\n",
    "        else:\n",
    "            i = i\n",
    "            \n",
    "        # apply the filter. For each variation the pass band is widened.\n",
    "    \n",
    "        bachfilt = voiceFilter(pitchesnest2[z][i], bachcut, z+20)\n",
    "    \n",
    "        # apply saturation\n",
    "        \n",
    "        bachfilt = bachfilt + badSaturator(bachfilt, bachfilt.max()/6)\n",
    "        \n",
    "        # apply sigmoid window\n",
    "        \n",
    "        bachfilt = bachfilt*sigmoidWindow(bachfilt.size, int((overlap)*(i+1)))\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "    \n",
    "        if z == 0 and i == 0:\n",
    "    \n",
    "            audio2 = np.concatenate((audio2, bachfilt))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            audio2 = np.concatenate((audio2, np.zeros(bachfilt.size-overlap*(i+1))))\n",
    "            audio2 = np.concatenate((audio2[:(audio2.size-bachfilt.size)], (audio2[(audio2.size-bachfilt.size)] + bachfilt)))\n",
    "\n",
    "    # create the tenor line\n",
    "    \n",
    "    for i, j in enumerate(tenorstarts):\n",
    "        \n",
    "        # slice the audio between the start and end of each note\n",
    "\n",
    "        bachcut = bach[int(j*sr):int(tenorends[i]*sr)+1]\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest3[z]):\n",
    "            i = i-c\n",
    "            c = c+1\n",
    "        else:\n",
    "            i = i\n",
    "    \n",
    "        # apply the filter. For each variation the pass band is widened.\n",
    "        \n",
    "        bachfilt = voiceFilter(pitchesnest3[z][i], bachcut, z+20)\n",
    "    \n",
    "        # apply saturation\n",
    "        \n",
    "        bachfilt = bachfilt + badSaturator(bachfilt, bachfilt.max()/6)\n",
    "        \n",
    "        # apply sigmoid window\n",
    "        \n",
    "        bachfilt = bachfilt*sigmoidWindow(bachfilt.size, int((overlap)*(i+1)))\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "    \n",
    "        if z == 0 and i == 0:\n",
    "    \n",
    "            audio3 = np.concatenate((audio3, bachfilt))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            audio3 = np.concatenate((audio3, np.zeros(bachfilt.size-overlap*(i+1))))\n",
    "            audio3 = np.concatenate(((audio3[:(audio3.size-bachfilt.size)], (audio3[(audio3.size-bachfilt.size)] + bachfilt))))\n",
    "\n",
    "    # create the bass line\n",
    "    \n",
    "    for i, j in enumerate(bassstarts):\n",
    "        \n",
    "        # slice the audio between the start and end of each note\n",
    "    \n",
    "        bachcut = bach[int(j*sr):int(bassends[i]*sr)+1]\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "    \n",
    "        if i >= len(pitchesnest4[z]):\n",
    "            i = i-d\n",
    "            d = d+1\n",
    "        else:\n",
    "            i = i\n",
    "        \n",
    "        # apply the filter. For each variation the pass band is widened.\n",
    "        \n",
    "        bachfilt = voiceFilter(pitchesnest4[z][i], bachcut, z+20)\n",
    "\n",
    "        # apply saturation\n",
    "        \n",
    "        bachfilt = bachfilt + badSaturator(bachfilt, bachfilt.max()/6)\n",
    "        \n",
    "        # apply sigmoid window\n",
    "        \n",
    "        bachfilt = bachfilt*sigmoidWindow(bachfilt.size, int((overlap)*(i+1)))\n",
    "        \n",
    "        # decide which pitch value index to take. If pitch index is above note value index it takes the final pitch\n",
    "        \n",
    "        if z == 0 and i == 0:\n",
    "    \n",
    "            audio4 = np.concatenate((audio4, bachfilt))\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            audio4 = np.concatenate((audio4, np.zeros(bachfilt.size-overlap*(i+1))))\n",
    "            audio4 = np.concatenate((audio4[:(audio4.size-bachfilt.size)], (audio4[(audio4.size-bachfilt.size)] + bachfilt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc065d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here I create the summed audio data. Due to the fact that each voice's individual data is not the same length\n",
    "# (but only by a few sample), I need to slice them to the same length.\n",
    "\n",
    "if audio.size < audio2.size and audio.size < audio3.size and audio.size < audio4.size:\n",
    "    \n",
    "    audio2 = audio2[:audio.size]\n",
    "    audio3 = audio3[:audio.size]\n",
    "    audio4 = audio4[:audio.size]\n",
    "    \n",
    "elif audio2.size < audio.size and audio2.size < audio3.size and audio2.size < audio4.size:\n",
    "    \n",
    "    audio = audio[:audio2.size]\n",
    "    audio3 = audio3[:audio2.size]\n",
    "    audio4 = audio4[:audio2.size]\n",
    "    \n",
    "elif audio3.size < audio.size and audio3.size < audio2.size and audio3.size < audio4.size:\n",
    "    \n",
    "    audio = audio[:audio3.size]\n",
    "    audio2 = audio2[:audio3.size]\n",
    "    audio4 = audio4[:audio3.size]\n",
    "\n",
    "    \n",
    "elif audio4.size < audio.size and audio4.size < audio2.size and audio4.size < audio3.size:\n",
    "    \n",
    "    audio = audio[:audio4.size]\n",
    "    audio2 = audio2[:audio4.size]\n",
    "    audio3 = audio3[:audio4.size]\n",
    "\n",
    "\n",
    "# sum the audio of the four voices\n",
    "\n",
    "audiofin = audio + audio2 + audio3 + audio4\n",
    "\n",
    "# normalise the audio\n",
    "\n",
    "audiofin = audiofin/np.max(audiofin)\n",
    "\n",
    "# plot the waveform and listen\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Waveform of Rearranged Audio')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(np.arange(0, audiofin.size, 1)/sr, audiofin)\n",
    "\n",
    "ipd.Audio(audiofin, rate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff248b54",
   "metadata": {},
   "source": [
    "This is admittedly not the healthiest looking waveform, but the sound is there. To fill things out a little, we will underlie this with sines, also generated according to the Midi data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed93f296",
   "metadata": {},
   "source": [
    "# Step 9: Create Sine Waves to Underlie the Audio\n",
    "\n",
    "The same basic process and architecture is used to generate the sines as was used to rearrange the audio.\n",
    "\n",
    "(A note on windows: With the splicing of the audio, the sigmoid window function that we created in combination with the overlap both removed clicks as well as avoided excessive amplitude \"pumping\". However, for the sines this window still resulted in clicking, even at greater overlaps. We also then attempted with the hanning window, and while this removed the clicks excessive pumping is still there. As we am running low on time, we have decided to stick with the hanning window and overlap, the lesser of two evils so to speak. For the portfolio we will attempt to truly solve this problem.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903faf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin = np.empty(0)\n",
    "sin2 = np.empty(0)\n",
    "sin3 = np.empty(0)\n",
    "sin4 = np.empty(0)\n",
    "\n",
    "for z in range(0, 4):\n",
    "    \n",
    "    # declare the values used to ensure that the last pitch value is held for excess notes\n",
    "    \n",
    "    a = 1\n",
    "    b = 1\n",
    "    c = 1\n",
    "    d = 1\n",
    "    \n",
    "    # create the soprano line\n",
    "    \n",
    "    for i, j in enumerate(sopranostarts):\n",
    "        \n",
    "        # here for if the number of notes exceeds number of pitches so as just to take final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest[z]):\n",
    "            \n",
    "            # generate sine\n",
    "                        \n",
    "            singen = sin_generator(0.25, (pitchesnest[z][i-a]), np.arange(0, sopranodurations[i], 1/sr))\n",
    "            \n",
    "            a = a+1\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # apply overlap\n",
    "            \n",
    "            sin = np.concatenate((sin, np.zeros(singen.size-(overlap)*(i+1)))) \n",
    "            \n",
    "            # add to final\n",
    "            \n",
    "            sin = np.concatenate((sin[:(sin.size-singen.size)], (sin[(sin.size-singen.size):] + singen)))\n",
    "\n",
    "        # here for rest of notes\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # generate sine\n",
    "            \n",
    "            singen = sin_generator(0.25, (pitchesnest[z][i]), np.arange(0, sopranodurations[i], 1/sr))\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # for first sine no need for overlap\n",
    "            \n",
    "            if z == 0 and i == 0:\n",
    "    \n",
    "                sin = np.concatenate((sin, singen))\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                # apply overlap\n",
    "        \n",
    "                sin = np.concatenate((sin, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "                # add to final\n",
    "                \n",
    "                sin = np.concatenate((sin[:(sin.size-singen.size)], (sin[(sin.size-singen.size):] + singen)))\n",
    "\n",
    "    \n",
    "   # create the alto line     \n",
    "        \n",
    "    for i, j in enumerate(altostarts):\n",
    "        \n",
    "        # here for if the number of notes exceeds number of pitches so as just to take final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest2[z]):\n",
    "            \n",
    "            # generate sine\n",
    "                        \n",
    "            singen = sin_generator(0.25, (pitchesnest2[z][i-b]), np.arange(0, altodurations[i], 1/sr))\n",
    "            \n",
    "            b = b+1\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # apply overlap\n",
    "            \n",
    "            sin2 = np.concatenate((sin2, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "            # add to final\n",
    "            \n",
    "            sin2 = np.concatenate((sin2[:(sin2.size-singen.size)], (sin2[(sin2.size-singen.size):] + singen)))\n",
    "\n",
    "        # here for rest of notes\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # generate sine\n",
    "            \n",
    "            singen = sin_generator(0.25, (pitchesnest2[z][i]), np.arange(0, altodurations[i], 1/sr))\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # for first sine no need for overlap\n",
    "            \n",
    "            if z == 0 and i == 0:\n",
    "    \n",
    "                sin2 = np.concatenate((sin2, singen))\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                # apply overlap\n",
    "        \n",
    "                sin2 = np.concatenate((sin2, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "                # add to final\n",
    "            \n",
    "                sin2 = np.concatenate((sin2[:(sin2.size-singen.size)], (sin2[(sin2.size-singen.size):] + singen)))\n",
    "            \n",
    "    # create the tenor line\n",
    "\n",
    "    for i, j in enumerate(tenorstarts):\n",
    "        \n",
    "        # here for if the number of notes exceeds number of pitches so as just to take final pitch        \n",
    "        \n",
    "        if i >= len(pitchesnest3[z]):\n",
    "            \n",
    "            # generate sine\n",
    "                        \n",
    "            singen = sin_generator(0.25, (pitchesnest3[z][i-c]), np.arange(0, tenordurations[i], 1/sr))\n",
    "            \n",
    "            c = c+1\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # apply overlap\n",
    "            \n",
    "            sin3 = np.concatenate((sin3, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "            # add to final\n",
    "            \n",
    "            sin3 = np.concatenate((sin3[:(sin3.size-singen.size)], (sin3[(sin3.size-singen.size):] + singen)))\n",
    "\n",
    "        # here for rest of notes\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # generate sine\n",
    "            \n",
    "            singen = sin_generator(0.25, (pitchesnest3[z][i]), np.arange(0, tenordurations[i], 1/sr))\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # for first sine no need for overlap\n",
    "            \n",
    "            if z == 0 and i == 0:\n",
    "    \n",
    "                sin3 = np.concatenate((sin3, singen))\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                # apply overlap\n",
    "        \n",
    "                sin3 = np.concatenate((sin3, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "                # add to final\n",
    "            \n",
    "                sin3 = np.concatenate((sin3[:(sin3.size-singen.size)], (sin3[(sin3.size-singen.size):] + singen)))\n",
    "            \n",
    "    # create the bass line        \n",
    "\n",
    "    for i, j in enumerate(bassstarts):\n",
    "        \n",
    "        # here for if the number of notes exceeds number of pitches so as just to take final pitch\n",
    "        \n",
    "        if i >= len(pitchesnest4[z]):\n",
    "            \n",
    "            # generate sine\n",
    "                        \n",
    "            singen = sin_generator(0.25, (pitchesnest4[z][i-d]), np.arange(0, bassdurations[i], 1/sr))\n",
    "            \n",
    "            d = d+1\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # apply overlap\n",
    "            \n",
    "            sin4 = np.concatenate((sin4, np.zeros(singen.size-(overlap*(i+1))))) \n",
    "            \n",
    "            # add to final\n",
    "            \n",
    "            sin4 = np.concatenate((sin4[:(sin4.size-singen.size)], (sin4[(sin4.size-singen.size):] + singen)))\n",
    "\n",
    "        # here for rest of notes\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # generate sine\n",
    "            \n",
    "            singen = sin_generator(0.25, (pitchesnest4[z][i]), np.arange(0, bassdurations[i], 1/sr))\n",
    "            \n",
    "            # apply window\n",
    "            \n",
    "            singen = singen*np.hanning(singen.size)\n",
    "            \n",
    "            # for first sine no need for overlap\n",
    "            \n",
    "            if z == 0 and i == 0:\n",
    "    \n",
    "                sin4 = np.concatenate((sin4, singen))\n",
    "        \n",
    "            else:\n",
    "                \n",
    "                # apply overlap\n",
    "        \n",
    "                sin4 = np.concatenate((sin4, np.zeros(singen.size-(overlap*(i+1)))))\n",
    "            \n",
    "                # add to final\n",
    "            \n",
    "                sin4 = np.concatenate((sin4[:(sin4.size-singen.size)], (sin4[(sin4.size-singen.size):] + singen)))\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1db737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I create the summed sine data. Due to the fact that each voice's individual data is not the same length\n",
    "# (but only by a few sample), I need to slice them to the same length.\n",
    "\n",
    "if sin.size < sin2.size and sin.size < sin3.size and sin.size < sin4.size:\n",
    "    \n",
    "    sin2 = sin2[:sin.size]\n",
    "    sin3 = sin3[:sin.size]\n",
    "    sin4 = sin4[:sin.size]\n",
    "    \n",
    "elif sin2.size < sin.size and sin2.size < sin3.size and sin2.size < sin4.size:\n",
    "    \n",
    "    sin = sin[:sin2.size]\n",
    "    sin3 = sin3[:sin2.size]\n",
    "    sin4 = sin4[:sin2.size]\n",
    "    \n",
    "elif sin3.size < sin.size and sin3.size < sin2.size and sin3.size < sin4.size:\n",
    "    \n",
    "    sin = sin[:sin3.size]\n",
    "    sin2 = sin2[:sin3.size]\n",
    "    sin4 = sin4[:sin3.size]\n",
    "\n",
    "    \n",
    "elif sin4.size < sin.size and sin4.size < sin2.size and sin4.size < sin3.size:\n",
    "    \n",
    "    sin = sin[:sin4.size]\n",
    "    sin2 = sin2[:sin4.size]\n",
    "    sin3 = sin3[:sin4.size]\n",
    "\n",
    "\n",
    "# sum the sines of the four voices\n",
    "\n",
    "sinfin = sin + sin2 + sin3 + sin4\n",
    "\n",
    "# normalise the sines\n",
    "\n",
    "sinfin = sinfin/np.max(sinfin)\n",
    "\n",
    "# plot the waveform and listen\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Waveform of Generated Sines')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(np.arange(0, sinfin.size, 1)/sr, sinfin)\n",
    "\n",
    "ipd.Audio(sinfin, rate = sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01c4f3",
   "metadata": {},
   "source": [
    "# Step 10: Combine the Audio and the Sines\n",
    "\n",
    "Although only differing in length by a couple of milliseconds, the audio and the sines are not identical in length. Therefore, the larger must be slightly trimmed before they are summed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4428b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check which is the larger data and slice\n",
    "\n",
    "if audiofin.size > sinfin.size:\n",
    "    audiofin = audiofin[:sinfin.size]\n",
    "else:\n",
    "    sinfin = sinfin[:audiofin.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "\n",
    "summed = audiofin + (sinfin*0.25)\n",
    "\n",
    "# cause waveform's so funky gonna have to do some magic to normalise\n",
    "\n",
    "# normalise positive side\n",
    "summed = summed/summed.max()\n",
    "\n",
    "# flip polarity\n",
    "summed = -summed\n",
    "\n",
    "# normalise negative side, as now positive\n",
    "summed = summed/summed.max()\n",
    "\n",
    "# plot the waveform\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Waveform of Summed Audio and Sines')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(np.arange(0, summed.size, 1)/sr, summed)\n",
    "\n",
    "ipd.Audio(summed, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232afaba",
   "metadata": {},
   "source": [
    "Again a bit of a strange waveform due to the filtering, but the sound is alright!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3fdeb",
   "metadata": {},
   "source": [
    "# Step 11: Send to PD for Processing\n",
    "\n",
    "Now we will send the audio to Pure Data for some processing. We will add some band pass filters with automated centre frequencies to create some movement, add some subtle reverb, and create a stereo wave file return.\n",
    "\n",
    "The audio will play as it processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e93083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the audio data to a wave file to be used in PD\n",
    "\n",
    "sf.write('./Files/audioMidiSum.wav', summed, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6afc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# choose the executable based upon operating system. If you have a completely different file path, you should copy\n",
    "# this in.\n",
    "\n",
    "# PC\n",
    "# pd_executable = '\"C:\\\\Program Files\\\\Pd\\\\bin\\\\pd.exe\"'\n",
    "\n",
    "# Mac\n",
    "pd_executable = '/Applications/Pd-0.51-4.app/Contents/Resources/bin/pd'\n",
    "\n",
    "# Linux\n",
    "#pd_executable = '/usr/local/bin/pd '\n",
    "\n",
    "pd_patch = './Files/ChoraleProcessor.pd'\n",
    "send = ' -send \";duration ' + str(summed.size/sr*1000) + '\"'\n",
    "\n",
    "command = pd_executable + ' -open ' + pd_patch + send + ' -r ' + str(sr) + ' -nogui'\n",
    "print(command)\n",
    "os.popen(command);\n",
    "\n",
    "time.sleep((summed.size/sr)+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f1c6f",
   "metadata": {},
   "source": [
    "We can now take a look at the waveform of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d57fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the processed audio\n",
    "\n",
    "dataprocessed = './Files/PDManip.wav'\n",
    "\n",
    "processed, sr = librosa.load(dataprocessed, sr = sr, mono=False)\n",
    "\n",
    "ipd.Audio(processed, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5459b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharey=True,  figsize=(14, 6))\n",
    "\n",
    "plt.subplots_adjust(hspace = .5)\n",
    "\n",
    "ax1.set_title('Waveform of Left Channel')\n",
    "ax1.set_xlabel('Time (Seconds)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.arange(0, ((processed[0].size-1)/sr), 1/sr), processed[0])\n",
    "\n",
    "ax2.set_title('Waveform of Right Channel')\n",
    "ax2.set_xlabel('Time (Seconds)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.plot(np.arange(0, ((processed[1].size-1)/sr), 1/sr), processed[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8891cefc",
   "metadata": {},
   "source": [
    "# Step 12: Apply Reverb through convolution\n",
    "\n",
    "To fill out the frequency spectrum again and create the textural effect that we wanted, we decided to also apply reverb through convolution.\n",
    "\n",
    "We decided to use the scipy convolution function, as it ran a lot faster than the numpy convolution version.\n",
    "\n",
    "After testing many different impulse responses, we settled upon one that we liked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import impulse response and listen\n",
    "\n",
    "dataimp = './Files/gozo_citadel_silo_ir_edit.wav'\n",
    "\n",
    "imp, sr = librosa.load(dataimp, sr = sr)\n",
    "\n",
    "ipd.Audio(imp, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the impulse response\n",
    "\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.title('Waveform of Impulse Response')\n",
    "plt.xlabel('Time (Seconds)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.plot(np.arange(0, imp.size, 1)/sr, imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b90a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convolve each channel and normalise\n",
    "\n",
    "finalleft = signal.convolve(processed[0], imp)\n",
    "finalleft = finalleft/finalleft.max()\n",
    "finalright = signal.convolve(processed[1], imp)\n",
    "finalright = finalright/finalright.max()\n",
    "\n",
    "# rejoin left and right channel\n",
    "\n",
    "final = np.vstack((finalleft, finalright))\n",
    "\n",
    "# listen to the final audio\n",
    "\n",
    "ipd.Audio(final, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61949e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot the simple waveforms of the left and right channel\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharey=True,  figsize=(14, 6))\n",
    "\n",
    "plt.subplots_adjust(hspace = .5)\n",
    "\n",
    "ax1.set_title('Waveform of Left Channel')\n",
    "ax1.set_xlabel('Time (Seconds)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.arange(0, ((finalleft.size)/sr), 1/sr), finalleft)\n",
    "\n",
    "ax2.set_title('Waveform of Right Channel')\n",
    "ax2.set_xlabel('Time (Seconds)')\n",
    "ax2.set_ylabel('Amplitude')\n",
    "ax2.plot(np.arange(0, ((finalright.size)/sr), 1/sr), finalright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b667e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for soundfile write\n",
    "\n",
    "finalReshape = final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a Wave File\n",
    "\n",
    "sf.write('./Files/BachRearrangedTexture.wav', finalReshape, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea693e07",
   "metadata": {},
   "source": [
    "We are pretty satisfied with the results. However, one issue that we do have is that due to the excessive filtering, there is little content above 800 Hz. For this reason we implemented the saturator, although it has to be driven extremely hard to fill the spectrum back up, and this doesn't sound very pleasant. Another way of dealing with this might be something think about for the portfolio.\n",
    "\n",
    "On the whole though, the results are a little different to what we were expecting when starting the project, but we like it nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260fdee",
   "metadata": {},
   "source": [
    "# Step 13: Waveforms\n",
    "\n",
    "We will now plot the waveforms of the final audio in two different styles:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9488800",
   "metadata": {},
   "source": [
    "First a Serato-Style waveform, where the colour of the waveform represents the spectral centroid. Red section have a lower spectral centroid, green sections a mid spectral centroid, and blue a higher spectral centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopLength = 512*3\n",
    "frameLength = 4096\n",
    "xTicksInterval = 10 #seconds between each tick. Decrease for smaller files, increase for larger files.\n",
    "\n",
    "colorThreshLow = 250 #in hz\n",
    "colorThreshHigh = 350 #in hz\n",
    "\n",
    "\n",
    "#Colors in HEX\n",
    "audacityColorScheme = {\n",
    "    'wave': '#312fc1',\n",
    "    'rms': '#6464df',\n",
    "    'bg': '#c0c1c1',\n",
    "}\n",
    "\n",
    "#Colors in RGBA\n",
    "seratoColorScheme = {\n",
    "    'high': (0,0,1,.5),\n",
    "    'mid': (0,1,0,.5),\n",
    "    'bass': (1,0,0,.5),\n",
    "    'bg': '#000000'\n",
    "}\n",
    "\n",
    "#Load Audio\n",
    "audioData = final\n",
    "\n",
    "#Split into two channels, easier to keep track.\n",
    "audioDataL = audioData[0]\n",
    "audioDataR = audioData[1]\n",
    "\n",
    "#RMS Left\n",
    "rmsL = librosa.feature.rms(audioDataL, frame_length=frameLength, hop_length=hopLength)\n",
    "rmsL = np.ndarray.flatten(rmsL)\n",
    "\n",
    "rmsLRepeat = np.repeat(rmsL, frameLength)\n",
    "appliedRMSL = np.multiply(audioDataL, rmsLRepeat[:audioDataL.size])\n",
    "\n",
    "#RMS Right\n",
    "rmsR = librosa.feature.rms(audioDataR, frame_length=frameLength, hop_length=hopLength)\n",
    "rmsR = np.ndarray.flatten(rmsR)\n",
    "\n",
    "rmsRRepeat = np.repeat(rmsR, frameLength)\n",
    "appliedRMSR = np.multiply(audioDataR, rmsRRepeat[:audioDataR.size])\n",
    "\n",
    "#Spectral Centroid Left\n",
    "centL = librosa.feature.spectral_centroid(audioDataL, n_fft=frameLength, hop_length=hopLength)\n",
    "centL = np.ndarray.flatten(centL)\n",
    "\n",
    "centLRepeat = np.repeat(centL, frameLength)\n",
    "appliedCentL = np.multiply(audioDataL, centLRepeat[:audioDataL.size])\n",
    "\n",
    "#Spectral Centroid Right\n",
    "centR = librosa.feature.spectral_centroid(audioDataR, n_fft=frameLength, hop_length=hopLength)\n",
    "\n",
    "centRRepeat = np.repeat(centR, frameLength)\n",
    "appliedCentR = np.multiply(audioDataR, centRRepeat[:audioDataR.size])\n",
    "\n",
    "lowCentL = np.zeros(audioDataL.size)\n",
    "midCentL = np.zeros(audioDataL.size)\n",
    "highCentL = np.zeros(audioDataL.size)\n",
    "lowCentR = np.zeros(audioDataR.size)\n",
    "midCentR = np.zeros(audioDataR.size)\n",
    "highCentR = np.zeros(audioDataR.size)\n",
    "\n",
    "#Isolate frequencies based on hoplength and centroid\n",
    "for i, iHop in enumerate(range(0, audioDataL.size, hopLength)):\n",
    "    if (centL[i] < colorThreshLow):\n",
    "        lowCentL[iHop:iHop+frameLength] = audioDataL[iHop:iHop+frameLength]\n",
    "        lowCentR[iHop:iHop+frameLength] = audioDataR[iHop:iHop+frameLength]\n",
    "    elif (centL[i] > colorThreshLow and centL[i] < colorThreshHigh):\n",
    "        midCentL[iHop:iHop+frameLength] = audioDataL[iHop:iHop+frameLength]\n",
    "        midCentR[iHop:iHop+frameLength] = audioDataR[iHop:iHop+frameLength]\n",
    "    else:\n",
    "        highCentL[iHop:iHop+frameLength] = audioDataL[iHop:iHop+frameLength]\n",
    "        highCentR[iHop:iHop+frameLength] = audioDataR[iHop:iHop+frameLength]\n",
    "\n",
    "        \n",
    "#Plot Serato Style\n",
    "fig, (axL, axR) = plt.subplots(2, figsize=[15,8])\n",
    "plt.xticks(np.arange(0, audioData.size, sr * xTicksInterval), np.arange(0, audioData.size/sr, xTicksInterval))\n",
    "#Left Channel\n",
    "axL.set_facecolor(seratoColorScheme[\"bg\"])\n",
    "axL.plot(lowCentL, color=seratoColorScheme[\"bass\"])\n",
    "axL.plot(midCentL, color=seratoColorScheme[\"mid\"])\n",
    "axL.plot(highCentL, color=seratoColorScheme[\"high\"])\n",
    "axL.set_ylabel(\"Amplitude (normalized -1 to 1)\")\n",
    "axL.set_xlabel(\"Time (sec)\")\n",
    "axL.set_title(\"Serato Style - Left Channel\")\n",
    "axL.set_xmargin(0)\n",
    "axL.sharex(axR)\n",
    "\n",
    "#Right Channel\n",
    "axR.set_facecolor(seratoColorScheme[\"bg\"])\n",
    "axR.plot(lowCentR, color=seratoColorScheme[\"bass\"])\n",
    "axR.plot(midCentR, color=seratoColorScheme[\"mid\"])\n",
    "axR.plot(highCentR, color=seratoColorScheme[\"high\"])\n",
    "axR.set_ylabel(\"Amplitude (normalized -1 to 1)\")\n",
    "axR.set_xlabel(\"Time (sec)\")\n",
    "axR.set_title(\"Serato Style - Right Channel\")\n",
    "axR.set_xmargin(0)\n",
    "fig.tight_layout() #Must be right before plt.show(), otherwise it scales the plots as well.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4608213",
   "metadata": {},
   "source": [
    "And secondly an Audacity style, where the inner waveform represents the RMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970798a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Audacity Style\n",
    "\n",
    "fig, (axL, axR) = plt.subplots(2, figsize=[15,6])\n",
    "plt.xticks(np.arange(0, audioData.size, sr * xTicksInterval), np.arange(0, audioData.size/sr, xTicksInterval))\n",
    "#Left Channel\n",
    "axL.set_prop_cycle(color=[audacityColorScheme[\"wave\"], audacityColorScheme[\"rms\"]])\n",
    "axL.set_facecolor(audacityColorScheme[\"bg\"])\n",
    "axL.plot(audioDataL)\n",
    "axL.plot(appliedRMSL)\n",
    "axL.legend(['Waveform', 'rms'], loc='upper right')\n",
    "axL.set_ylabel(\"Amplitude (normalized -1 to 1)\")\n",
    "axL.set_xlabel(\"Time (sec)\")\n",
    "axL.set_title(\"Audacity Style - Left Channel\")\n",
    "axL.set_xmargin(0)\n",
    "axL.sharex(axR)\n",
    "\n",
    "#Right Channel\n",
    "axR.set_prop_cycle(color=[audacityColorScheme[\"wave\"], audacityColorScheme[\"rms\"]])\n",
    "axR.set_facecolor(audacityColorScheme[\"bg\"])\n",
    "axR.plot(audioDataR)\n",
    "axR.plot(appliedRMSR)\n",
    "axR.legend(['Waveform', 'rms'], loc='upper right')\n",
    "axR.set_ylabel(\"Amplitude (normalized -1 to 1)\")\n",
    "axR.set_xlabel(\"Time (sec)\")\n",
    "axR.set_title(\"Audacity Style - Right Channel\")\n",
    "axR.set_xmargin(0)\n",
    "fig.tight_layout() #Must be right before plt.show(), otherwise it scales the plots as well.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
